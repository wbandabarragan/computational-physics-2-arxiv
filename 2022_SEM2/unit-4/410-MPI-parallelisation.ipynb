{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d639e3da",
   "metadata": {},
   "source": [
    "# MPI parallelisation\n",
    "\n",
    "We will need to install two packages:\n",
    "\n",
    "- The actual MPI library:\n",
    "\n",
    "```\n",
    "conda install openmpi\n",
    "```\n",
    "\n",
    "Or: \n",
    "\n",
    "```\n",
    "conda install -c conda-forge openmpi\n",
    "```\n",
    "\n",
    "- The python MPI package:\n",
    "```\n",
    "conda install mpi4py\n",
    "```\n",
    "\n",
    "\n",
    "## 1. Basic python script:\n",
    "\n",
    "Let's create a .py script that writes \"Hello World\":\n",
    "\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Hello World!\")\n",
    "    \n",
    "```\n",
    "\n",
    "To execute it in a terminal:\n",
    "\n",
    "```\n",
    "wladimir$ python example_mpi1.py\n",
    "\n",
    "Hello World!\n",
    "```\n",
    "\n",
    "\n",
    "## 2. Getting Started with MPI\n",
    "\n",
    "Let’s try running this code on multiple processes. This is done using the **mpiexec** command.\n",
    "\n",
    "Many environments also provide an **mpirun** command, which usually - but not always - works the same way. \n",
    "\n",
    "Whenever possible, you should use mpiexec and not mpirun, in order to guarantee more consistent results.\n",
    "\n",
    "### MPI - mpiexec vs mpirun\n",
    "\n",
    "- MPI stands for **‘message passing interface’** and is a message passing standard which is designed to work on a variety of parallel computing architectures.\n",
    "\n",
    "\n",
    "- The MPI standard defines how syntax and semantics of a library of routines. There are a number of implementations of this standard including OpenMPI, MPICH, and MS MPI.\n",
    "\n",
    "\n",
    "- The primary difference between mpiexec and mpirun is that mpiexec is defined as part of the MPI standard, while mpirun is not.\n",
    "\n",
    "\n",
    "- Different implementations of MPI (i.e. OpenMPI, MPICH, MS MPI, etc.) are not guaranteed to implement mpirun, or might implement different options for mpirun.\n",
    "\n",
    "- Technically, the MPI standard doesn’t actually require that MPI implementations implement mpiexec either, but the standard does at least describe guidelines for how mpiexec should work. Because of this, mpiexec is generally the preferred command.\n",
    "\n",
    "\n",
    "The general format for launching a code on multiple processes is:\n",
    "\n",
    "Let's create a **example2.py** script that writes \"Hello World\" and execute it in a terminal:\n",
    "\n",
    "\n",
    "```\n",
    "wladimir$ mpiexec -n 2 python example_mpi1.py\n",
    "Hello World!\n",
    "Hello World! \n",
    "```\n",
    "\n",
    "When you execute the above command, mpiexec launches 2 different instances of python example_mpi1.py simultaneously, which each print “Hello World!”.\n",
    "\n",
    "Typically, as long as you have at least 2 processors on the machine you are running on, each process will be launched on a different processor; however, certain environment variables and optional arguments to mpiexec can change this behavior. Each process runs the code in example_mpi1.py independently of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f017df6",
   "metadata": {},
   "source": [
    "### MPI communicator:\n",
    "\n",
    "It might not be obvious yet, but the processes mpiexec launches aren’t completely unaware of one another. The mpiexec adds each of the processes to an MPI communicator, which enables each of the processes to send and receive information to one another via MPI. The MPI communicator that spans all of the processes launched by mpiexec is called **MPI.COMM_WORLD.**\n",
    "\n",
    "In mpi4py, communicators are class objects, and we can query information about them through their class functions. Edit example_mpi2.py so that it reads as follows:\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    world_comm = MPI.COMM_WORLD\n",
    "    world_size = world_comm.Get_size()\n",
    "    my_rank = world_comm.Get_rank()\n",
    "\n",
    "    print(\"World Size: \" + str(world_size) + \"   \" + \"Rank: \" + str(my_rank))\n",
    "```\n",
    "\n",
    "In the above code we first import mpi4py. Then, we get the communicator that spans all of the processes, which is called **MPI.COMM_WORLD.**\n",
    "\n",
    "The communicator’s **Get_size()** function tells us the total number of processes within that communicator. Each of these processes is assigned a unique rank, which is an integer that ranges from **0** to **world_size - 1**.\n",
    "\n",
    "The rank of a process allows it to be identified whenever processes communicate with one another. For example, in some cases we might want rank 2 to send some information to rank 4, or we might want rank 0 to receive information from all of the other processes.\n",
    "\n",
    "Calling **world_comm.Get_rank()** returns the rank of the process that called it within world_comm.\n",
    "\n",
    "\n",
    "Then, we execute it in a terminal:\n",
    "\n",
    "```\n",
    "wladimir$ mpiexec -n 2 python example_mpi2.py\n",
    "World Size: 2   Rank: 0\n",
    "World Size: 2   Rank: 1\n",
    "```\n",
    "\n",
    "\n",
    "As we can see, the **world_comm.Get_size()** function returns 2, which is the total number of ranks we told mpiexec to run with (through the -n argument). Each of the processes is assigned a rank in the range of 0 to 1.\n",
    "\n",
    "The ranks may not necessarily print out their messages in order; whichever rank reaches the print function first will print out its message first. If you run the code again with more processors, the ranks are likely to print their messages in a different order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88fc5b0c",
   "metadata": {},
   "source": [
    "## 3. Basic Infrastructure\n",
    "\n",
    "We will now do some work with the script in **example_mpi3.py**, which does some simple math with NumPy arrays:\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    N = 10000000\n",
    "\n",
    "    # initialize a\n",
    "    a = np.ones( N )\n",
    "\n",
    "    # initialize b\n",
    "    b = np.zeros( N )\n",
    "    for i in range( N ):\n",
    "        b[i] = 1.0 + i\n",
    "\n",
    "    # add the two arrays\n",
    "    for i in range( N ):\n",
    "        a[i] = a[i] + b[i]\n",
    "\n",
    "    # average the result\n",
    "    sum = 0.0\n",
    "    for i in range( N ):\n",
    "        sum += a[i]\n",
    "    average = sum / N\n",
    "\n",
    "    print(\"Average: \" + str(average))\n",
    "```\n",
    "\n",
    "\n",
    "Then, run it:\n",
    "\n",
    "```\n",
    "wladimir$ python example_mpi3.py\n",
    "Average: 5000001.5\n",
    "```\n",
    "\n",
    "### MPI wall time\n",
    "\n",
    "Let’s learn something about which parts of this code account for most of the run time. **mpi4py** provides a timer, **MPI.Wtime()**, which returns the current walltime. We can use this function to determine how long each section of the code takes to run.\n",
    "\n",
    "For example, to determine how much time is spent initializing array **a**, do the following in example_mpi4.py:\n",
    "\n",
    "```python \n",
    "    # initialize a\n",
    "    start_time = MPI.Wtime()\n",
    "    a = np.ones(N)\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Initialize a time: \" + str(end_time-start_time))\n",
    "```\n",
    "\n",
    "As the above code indicates, we don’t really want every rank to print the timings, since that could look messy in the output. Instead, we have only rank 0 print this information. Of course, this requires that we add a few lines near the top of the code to query the rank of each process:\n",
    "\n",
    "```python\n",
    "    # get basic information about the MPI communicator\n",
    "    world_comm = MPI.COMM_WORLD\n",
    "    world_size = world_comm.Get_size()\n",
    "    my_rank = world_comm.Get_rank()\n",
    "    \n",
    "```\n",
    "\n",
    "Also determine and print the timings of each of the other sections of the code: the intialization of array **b**, the addition of the two arrays, and the final averaging of the result. Your code should look something like this:\n",
    "\n",
    "```python \n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "# get basic information about the MPI communicator\n",
    "world_comm = MPI.COMM_WORLD\n",
    "world_size = world_comm.Get_size()\n",
    "my_rank = world_comm.Get_rank()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    N = 10000000\n",
    "\n",
    "    # initialize a\n",
    "    start_time = MPI.Wtime() # Adding time stamp\n",
    "    a = np.ones(N)\n",
    "    end_time   = MPI.Wtime() # Adding time stamp\n",
    "\n",
    "    # Print the time\n",
    "    if my_rank == 0:\n",
    "        print(\"Initialise a time: \" + str(end_time - start_time))\n",
    "\n",
    "    # initialize b\n",
    "    start_time1 = MPI.Wtime() # Adding time stamp\n",
    "    b = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        b[i] = 1.0 + i\n",
    "    end_time1    = MPI.Wtime() # Adding time stamp\n",
    "\n",
    "    # Print the time\n",
    "    if my_rank == 0:\n",
    "        print(\"Initialise b time: \" + str(end_time1 - start_time1))\n",
    "\n",
    "    # add the two arrays\n",
    "    start_time2 = MPI.Wtime() # Adding time stamp\n",
    "    for i in range(N):\n",
    "        a[i] = a[i] + b[i]\n",
    "    end_time2 = MPI.Wtime() # Adding time stamp\n",
    "\n",
    "    # Print the time\n",
    "    if my_rank == 0:\n",
    "        print(\"Adding arrays time: \" + str(end_time2 - start_time2))\n",
    "\n",
    "    # average the result\n",
    "    start_time3 = MPI.Wtime() # Adding time stamp\n",
    "    sum = 0.0\n",
    "    for i in range(N):\n",
    "        sum += a[i]\n",
    "    average = sum / N\n",
    "    end_time3 = MPI.Wtime() # Adding time stamp\n",
    "\n",
    "    if my_rank == 0:\n",
    "        print(\"Averaging result time: \" + str(end_time3 - start_time3))\n",
    "        print(\"Average: \" + str(average))```\n",
    "```\n",
    "\n",
    "Run this with one and several cores:\n",
    "\n",
    "#### Serial run:\n",
    "```\n",
    "wladimir$ python example_mpi4.py \n",
    "Initialise a time: 0.03160300000000005\n",
    "Initialise b time: 1.683021\n",
    "Adding arrays time: 3.578656\n",
    "Averaging result time: 2.1059520000000003\n",
    "Average: 5000001.5\n",
    "```\n",
    "\n",
    "#### Parallel Run:\n",
    "```\n",
    "mpirun -n 2 python example_mpi4.py \n",
    "Initialise a time: 0.033883\n",
    "Initialise b time: 1.6831939999999999\n",
    "Adding arrays time: 3.9273849999999997\n",
    "Averaging result time: 2.986340000000001\n",
    "Average: 5000001.5\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d61541f0",
   "metadata": {},
   "source": [
    "## 4. Point-to-Point Communication\n",
    "\n",
    "We can see that running on multiple ranks doesn’t help with the timings, because each rank is duplicating all of the same work.\n",
    "\n",
    "We want the ranks to cooperate on the problem, with each rank working on a different part of the calculation. In this example, that means that different ranks will work on different parts of the arrays **a** and **b**, and then the results on each rank will be summed across all the ranks.\n",
    "\n",
    "We need to decide what parts of the arrays each of the ranks will work on; this is more generally known as a rank’s workload. Add the following code just before the initialization of array **a** in example_mpi6.py:\n",
    "\n",
    "```python \n",
    "    # determine the workload of each rank\n",
    "    workloads = [ N // world_size for i in range(world_size) ]\n",
    "    for i in range( N % world_size ):\n",
    "        workloads[i] += 1\n",
    "    my_start = 0\n",
    "    for i in range( my_rank ):\n",
    "        my_start += workloads[i]\n",
    "    my_end = my_start + workloads[my_rank]\n",
    "```\n",
    "\n",
    "In the above code, **my_start** and **my_end** represent the range over which each rank will perform mathematical operations on the arrays.\n",
    "\n",
    "#### Serial Run: \n",
    "\n",
    "```\n",
    "wladimir$ python example_mpi5.py \n",
    "Printing workloads: [10000000]\n",
    "Printing my_start: 0\n",
    "Printing my_end: 10000000\n",
    "Initialise a time: 0.055971\n",
    "Initialise b time: 1.6320759999999999\n",
    "Adding arrays time: 3.598573\n",
    "Averaging result time: 1.9293189999999996\n",
    "Average: 5000001.5\n",
    "```\n",
    "\n",
    "#### Parallel Run: \n",
    "\n",
    "```\n",
    "mpirun -n 2 python example_mpi5.py \n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 5000000\n",
    "Printing my_end: 10000000\n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 0\n",
    "Printing my_end: 5000000\n",
    "Initialise a time: 0.111718\n",
    "Initialise b time: 1.808382\n",
    "Adding arrays time: 4.003725\n",
    "Averaging result time: 2.0773859999999997\n",
    "Average: 5000001.5\n",
    "```\n",
    "\n",
    "\n",
    "This script is still not helping with times, so we’ll start by parallelizing the code that averages the result in example_mpi6.py. Update the range of the for loop in this part of the code to the following:\n",
    "\n",
    "```python \n",
    "    for i in range( my_start, my_end ):\n",
    "    \n",
    "```\n",
    "\n",
    "This will ensure that each rank is only calculating elements **my_start** through **my_end** of the sum. We then need the ranks to communicate their individually calculated sums so that we can calculate the global sum. To do this, replace the line **average = sum / N** with:\n",
    "\n",
    "\n",
    "```python \n",
    "    if my_rank == 0:\n",
    "        world_sum = sum\n",
    "        for i in range( 1, world_size ):\n",
    "      \t    sum_np = np.empty( 1 )\n",
    "            world_comm.Recv( [sum_np, MPI.DOUBLE], source=i, tag=77 )\n",
    "            world_sum += sum_np[0]\n",
    "        average = world_sum / N\n",
    "    else:\n",
    "        sum_np = np.array( [sum] )\n",
    "        world_comm.Send( [sum_np, MPI.DOUBLE], dest=0, tag=77 )\n",
    "```\n",
    "\n",
    "The **MPI.DOUBLE** parameter tells MPI what type of information is being communicated by the **Send** and **Recv** calls. In this case, we are sending an array of double precision numbers. If you are communicating information of a different datatype, consult the following:\n",
    "\n",
    "\n",
    "\n",
    "#### Serial run:\n",
    "```\n",
    "wladimir$ python example_mpi4.py \n",
    "Initialise a time: 0.03160300000000005\n",
    "Initialise b time: 1.683021\n",
    "Adding arrays time: 3.578656\n",
    "Averaging result time: 2.1059520000000003\n",
    "Average: 5000001.5\n",
    "```\n",
    "\n",
    "#### Parallel run:\n",
    "\n",
    "```\n",
    "$ mpirun -n 2 python example_mpi6.py \n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 0\n",
    "Printing my_end: 5000000\n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 5000000\n",
    "Printing my_end: 10000000\n",
    "Initialise a time: 0.049703\n",
    "Initialise b time: 1.733414\n",
    "Adding arrays time: 3.9768729999999994\n",
    "Averaging result time: 0.9668559999999999\n",
    "Average: 5000001.5\n",
    "```\n",
    "\n",
    "You can see that the amount of time spent calculating the average has indeed gone down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da6ec7",
   "metadata": {},
   "source": [
    "### Parallelisation of the sum of two arrays:\n",
    "\n",
    "Parallelizing the part of the code that adds the two arrays is much easier. All we need to do is update the range over which the for loop iterates:\n",
    "\n",
    "```python\n",
    "    for i in range(my_start, my_end):\n",
    "```\n",
    "\n",
    "#### Comparison:\n",
    "\n",
    "```\n",
    "wladimir$ mpirun -n 2 python example_mpi6.py \n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 0\n",
    "Printing my_end: 5000000\n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 5000000\n",
    "Printing my_end: 10000000\n",
    "Initialise a time: 0.033704\n",
    "Initialise b time: 1.729189\n",
    "Adding arrays time: 3.9222249999999996\n",
    "Averaging result time: 1.2841199999999997\n",
    "Average: 5000001.5\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "wladimir$ mpirun -n 2 python example_mpi7.py \n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 0\n",
    "Printing my_end: 5000000\n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 5000000\n",
    "Printing my_end: 10000000\n",
    "Initialise a time: 0.033571\n",
    "Initialise b time: 1.8027939999999998\n",
    "Adding arrays time: 1.8451830000000002\n",
    "Averaging result time: 1.4243779999999995\n",
    "Average: 5000001.5\n",
    "```\n",
    "\n",
    "The array addition time has gone down nicely. Surprisingly enough, the most expensive part of the calculation is now the initialization of array b. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a6539",
   "metadata": {},
   "source": [
    "### Parallelisation of the initialisation of b:\n",
    "\n",
    "Updating the range over which that loop iterates speeds up that part of the calculation:\n",
    "\n",
    "```python\n",
    "    for i in range(my_start, my_end):\n",
    "```\n",
    "\n",
    "#### Comparison:\n",
    "\n",
    "```\n",
    "wladimir$ mpirun -n 2 python example_mpi7.py \n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 0\n",
    "Printing my_end: 5000000\n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 5000000\n",
    "Printing my_end: 10000000\n",
    "Initialise a time: 0.033571\n",
    "Initialise b time: 1.8027939999999998\n",
    "Adding arrays time: 1.8451830000000002\n",
    "Averaging result time: 1.4243779999999995\n",
    "Average: 5000001.5\n",
    "```\n",
    "\n",
    "```\n",
    "wladimir$ mpirun -n 2 python example_mpi8.py \n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 0\n",
    "Printing my_end: 5000000\n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 5000000\n",
    "Printing my_end: 10000000\n",
    "Initialise a time: 0.066303\n",
    "Initialise b time: 0.848179\n",
    "Adding arrays time: 1.8135150000000002\n",
    "Averaging result time: 1.1754170000000004\n",
    "Average: 5000001.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83aa76",
   "metadata": {},
   "source": [
    "### Reducing the Memory Footprint\n",
    "\n",
    "The simulation is running much faster now thanks to the parallelization we have added. If that’s all we care about, we could stop working on the code now.\n",
    "\n",
    "In reality, though, time is only one resource we should be concerned about. Another resource that is often even more important is memory.\n",
    "\n",
    "The changes we have made to the code make it run faster, but don’t decrease its memory footprint in any way: \n",
    "\n",
    "\n",
    "- Each rank allocates arrays **a** and **b** with **N** double precision values.\n",
    "\n",
    "\n",
    "- That means that each rank allocates **2N** double precision values; across all of our ranks, that corresponds to a total of **2 * nproc * world_size** double precision values.\n",
    "\n",
    "\n",
    "- **Running on more processors might decrease our run time, but it increases our memory footprint!**\n",
    "\n",
    "Of course, there isn't really a good reason for each rank to allocate the entire arrays of size **N**, because each rank will only ever use values within the range of **my_start** to **my_end**. Let’s modify the code so that each rank allocates **a** and **b** to a size of **workloads[my_rank]**.\n",
    "\n",
    "\n",
    "Replace the initialization of **a** with:\n",
    "\n",
    "```\n",
    "    a = np.ones( workloads[my_rank] )\n",
    "```\n",
    "Replace the initialization of b with:\n",
    "\n",
    "```\n",
    "    b = np.zeros( workloads[my_rank] )\n",
    "    for i in range( workloads[my_rank] ):\n",
    "        b[i] = 1.0 + ( i + my_start )\n",
    "```\n",
    "\n",
    "Replace the range of the loops that add and sum the arrays to **range(workloads[my_rank])**.\n",
    "\n",
    "\n",
    "Then, run the code:\n",
    "\n",
    "```\n",
    "wladimir$ mpirun -np 2 python example_mpi9.py\n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 0\n",
    "Printing my_end: 5000000\n",
    "Printing workloads: [5000000, 5000000]\n",
    "Printing my_start: 5000000\n",
    "Printing my_end: 10000000\n",
    "Initialise a time: 0.018729\n",
    "Initialise b time: 1.109134\n",
    "Adding arrays time: 2.01623\n",
    "Averaging result time: 1.0165069999999998\n",
    "Average: 5000001.5\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "964a77a3",
   "metadata": {},
   "source": [
    "### Collective Communication\n",
    "\n",
    "Previously, we used point-to-point communication (i.e. **Send** and **Recv**) to sum the results across all ranks:\n",
    "\n",
    "```python\n",
    "    if my_rank == 0:\n",
    "        world_sum = sum\n",
    "        for i in range( 1, world_size ):\n",
    "            sum_np = np.empty( 1 )\n",
    "            world_comm.Recv( [sum_np, MPI.DOUBLE], source=i, tag=77 )\n",
    "            world_sum += sum_np[0]\n",
    "        average = world_sum / N\n",
    "    else:\n",
    "        sum_np = np.array( [sum] )\n",
    "        world_comm.Send( [sum_np, MPI.DOUBLE], dest=0, tag=77 )\n",
    "```\n",
    "\n",
    "MPI provides many collective communication functions, which automate many processes that can be complicated to write out using only point-to-point communication.\n",
    "\n",
    "### Reduce function:\n",
    "\n",
    "In particular, the **Reduce** function allows us to sum a value across all ranks, without all of the above code. Replace the above with:\n",
    "\n",
    "```python\n",
    "    sum = np.array( [sum] )\n",
    "    world_sum = np.zeros( 1 )\n",
    "    world_comm.Reduce( [sum, MPI.DOUBLE], [world_sum, MPI.DOUBLE], op = MPI.SUM, root = 0 )\n",
    "    average = world_sum / N\n",
    "```\n",
    "\n",
    "The **op** argument lets us specify what operation should be performed on all of the data that is reduced. Setting this argument to **MPI.SUM**, as we do above, causes all of the values to be summed onto the root process. There are many other operations provided by **MPI**, as you can see here:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note that in addition to enabling us to write simpler-looking code, collective communication operations tend to be faster than what we can achieve by trying to write our own communication operations using point-to-point calls.\n",
    "\n",
    "The code should look like this in **example_mpi10.py**:\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # get basic information about the MPI communicator\n",
    "    world_comm = MPI.COMM_WORLD\n",
    "    world_size = world_comm.Get_size()\n",
    "    my_rank = world_comm.Get_rank()\n",
    "\n",
    "    N = 10000000\n",
    "\n",
    "    # determine the workload of each rank\n",
    "    workloads = [ N // world_size for i in range(world_size) ]\n",
    "    for i in range( N % world_size ):\n",
    "        workloads[i] += 1\n",
    "    my_start = 0\n",
    "    for i in range( my_rank ):\n",
    "        my_start += workloads[i]\n",
    "    my_end = my_start + workloads[my_rank]\n",
    "\n",
    "    # initialize a\n",
    "    start_time = MPI.Wtime()\n",
    "    a = np.ones( workloads[my_rank] )\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Initialize a time: \" + str(end_time-start_time))\n",
    "\n",
    "    # initialize b\n",
    "    start_time = MPI.Wtime()\n",
    "    b = np.zeros( workloads[my_rank] )\n",
    "    for i in range( workloads[my_rank] ):\n",
    "        b[i] = 1.0 + ( i + my_start )\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Initialize b time: \" + str(end_time-start_time))\n",
    "\n",
    "    # add the two arrays\n",
    "    start_time = MPI.Wtime()\n",
    "    for i in range( workloads[my_rank] ):\n",
    "        a[i] = a[i] + b[i]\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Add arrays time: \" + str(end_time-start_time))\n",
    "\n",
    "    # average the result\n",
    "    start_time = MPI.Wtime()\n",
    "    sum = 0.0\n",
    "    for i in range( workloads[my_rank] ):\n",
    "        sum += a[i]\n",
    "    sum = np.array( [sum] )\n",
    "    world_sum = np.zeros( 1 )\n",
    "    world_comm.Reduce( [sum, MPI.DOUBLE], [world_sum, MPI.DOUBLE], op = MPI.SUM, root = 0 )\n",
    "    average = world_sum / N\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Average result time: \" + str(end_time-start_time))\n",
    "        print(\"Average: \" + str(average))\n",
    "```\n",
    "\n",
    "Then, we run the code again:\n",
    "\n",
    "```\n",
    "wladimir$ mpirun -np 2 python example_mpi10.py\n",
    "Initialize a time: 0.031435\n",
    "Initialize b time: 1.088035\n",
    "Add arrays time: 1.8827329999999998\n",
    "Average result time: 1.1045690000000001\n",
    "Average: [5000001.5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4738acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
